{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and specify local GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "import EWOthello.utils.plot_helpers as plt_util\n",
    "from EWOthello.data.othello import *\n",
    "from EWOthello.mingpt.dataset import ProbingDataset, CharDataset # AK's mingpt data child \n",
    "from EWOthello.mingpt.model import GPT, GPTConfig, GPTforProbing, GPTforProbing_v2\n",
    "from EWOthello.mingpt.probe_trainer import Trainer, TrainerConfig\n",
    "from EWOthello.mingpt.utils import set_seed, sample # AKs helpers for sampling predictions\n",
    "from EWOthello.mingpt.probe_model import BatteryProbeClassification\n",
    "set_seed(44)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "# device = \"mps\"\n",
    "device = torch.cuda.current_device()\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generation of Probe Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max num files: 230; Use_num: 1\n",
      "['gen10e5__20220324_155234.pickle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mem Used: 0.2633 GB: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n",
      "Deduplicating finished with 100000 games left\n",
      "Using 20 million for training, 0 for validation\n",
      "Dataset created has 100000 sequences, 61 unique words.\n"
     ]
    }
   ],
   "source": [
    "othello = get(ood_num=-1, data_root=None, num_preload=1) # 11 corresponds to over 1 million games\n",
    "game_dataset = CharDataset(othello) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=0'>1</a>\u001b[0m mconf \u001b[39m=\u001b[39m GPTConfig(game_dataset\u001b[39m.\u001b[39mvocab_size, game_dataset\u001b[39m.\u001b[39mblock_size, n_layer\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, n_head\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, n_embd\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=1'>2</a>\u001b[0m model_probe \u001b[39m=\u001b[39m GPTforProbing(mconf, probe_layer\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=2'>3</a>\u001b[0m model_probe\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m../EWOthello/ckpts/DeanKLi_GPT_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m))   \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=3'>4</a>\u001b[0m model_probe \u001b[39m=\u001b[39m model_probe\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=5'>6</a>\u001b[0m properties_modifier_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m59\u001b[39m, \u001b[39m64\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:152\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 152\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    153\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    154\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:136\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    133\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    139\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    140\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(game_dataset.vocab_size, game_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model_probe = GPTforProbing(mconf, probe_layer=6)\n",
    "model_probe.load_state_dict(torch.load(\"../EWOthello/ckpts/DeanKLi_GPT_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\"))   \n",
    "model_probe = model_probe.to(device)\n",
    "\n",
    "properties_modifier_matrix = np.ones((59, 64))\n",
    "for i in range(59):\n",
    "    if i % 2 == 1:\n",
    "        properties_modifier_matrix[i, :] *= -1.0\n",
    "\n",
    "property_container_v2 = []\n",
    "property_container = []\n",
    "act_container = []\n",
    "for x, _ in tqdm(game_dataset):\n",
    "    # Convert the game index sequence to board number sequence for use with the othello board class\n",
    "    tbf = [game_dataset.itos[_] for _ in x.tolist()]\n",
    "    valid_until = tbf.index(-100) if -100 in tbf else 999\n",
    "\n",
    "    # Get the board state vectors\n",
    "    a = OthelloBoardState()\n",
    "    properties = a.get_gt(tbf[:valid_until], \"get_state\")\n",
    "    property_container.extend(properties)\n",
    "    properties_v2 = (np.array(properties) - 1.0) * properties_modifier_matrix[:valid_until, :] + 1.0\n",
    "    property_container_v2.extend(properties_v2.tolist())\n",
    "\n",
    "    # Get the activation vectors\n",
    "    act = model_probe(x[None, :].to(device))[0].detach().cpu().split(1, dim=0)    \n",
    "    act_container.extend(act)\n",
    "    break\n",
    "\n",
    "### Visualize\n",
    "num_disp = 5\n",
    "fig = plt.figure(figsize=(num_disp*2, 4))\n",
    "ax = plt_util.addAxis(fig, 2, num_disp)\n",
    "for i in range(num_disp):\n",
    "    plt_util.plot_game_discs(np.reshape(property_container[i],[8,8]), ax[i])\n",
    "    plt_util.plot_game_discs(np.reshape(property_container_v2[i], [8,8]), ax[i+num_disp])\n",
    "    ax[i].set_title(f\"Moves Played: {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "ax[0].set_ylabel(\"Board State (Old)\")\n",
    "ax[num_disp].set_ylabel(\"New Representation\")\n",
    "plt_util.format_ax_boardImage(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train probes for all GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max num files: 230; Use_num: 11\n",
      "['gen10e5__20220324_165952.pickle', 'gen10e5__20220324_154919.pickle', 'gen10e5__20220324_164123.pickle', 'gen10e5__20220324_154043.pickle', 'gen10e5__20220324_155251.pickle', 'gen10e5__20220324_160016.pickle', 'gen10e5__20220324_165748.pickle', 'gen10e5__20220324_154002.pickle', 'gen10e5__20220324_155241.pickle', 'gen10e5__20220324_165707.pickle', 'gen10e5__20220324_160046.pickle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mem Used: 4.045 GB: 100%|██████████| 11/11 [00:05<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n",
      "Deduplicating finished with 1099941 games left\n",
      "Using 20 million for training, 0 for validation\n",
      "Dataset created has 1099941 sequences, 61 unique words.\n"
     ]
    }
   ],
   "source": [
    "othello = get(ood_num=-1, data_root=None, num_preload=11) # 11 corresponds to over 1 million games\n",
    "game_dataset = CharDataset(othello)\n",
    "\n",
    "class probe_dataset(Dataset):\n",
    "    def __init__(self, mconf, gpt_ckpt_path, game_dataset, probe_layer, property_type=\"new\"):\n",
    "        self.game_dataset = game_dataset\n",
    "        self.property_type = property_type # 'old' vs 'new'\n",
    "        self.property_modifier = np.concatenate([np.ones((1,64))*(-1)**i for i in range(59)],axis=0)\n",
    "\n",
    "        # Define the GPT probe model to return activations\n",
    "        GPT_probe = GPTforProbing(mconf, probe_layer=probe_layer)\n",
    "        GPT_probe.load_state_dict(torch.load(gpt_ckpt_path))\n",
    "        print(\"Loaded GPT Model from: \", gpt_ckpt_path)\n",
    "        for param in GPT_probe.parameters():\n",
    "            param.requires_grad = False\n",
    "        GPT_probe.eval()\n",
    "        self.GPT_probe = GPT_probe.to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.game_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, _ = self.game_dataset[index]\n",
    "        tbf = [game_dataset.itos[_] for _ in x.tolist()]\n",
    "        valid_until = tbf.index(-100) if -100 in tbf else 999\n",
    "\n",
    "        # Get the board state vectors\n",
    "        a = OthelloBoardState()\n",
    "        board_state = a.get_gt(tbf[:valid_until], \"get_state\")\n",
    "        if self.property_type==\"new\":\n",
    "            board_state = (np.array(board_state) - 1.0) * self.property_modifier[:valid_until, :] + 1.0\n",
    "\n",
    "        # Get the activation vectors\n",
    "        act = self.GPT_probe(x[None, :].to(device))[0, :valid_until, :].detach().cpu()\n",
    "        \n",
    "        return act, torch.tensor(board_state, dtype=torch.float32)\n",
    "\n",
    "def train_linear_probe(training_data, model_name, probe_layer, savepath, property_type=\"New\", num_epochs=1, train_to=1000, save_at_steps=100, verbose=False):\n",
    "    probe = BatteryProbeClassification(device, probe_class=3, num_task=64, input_dim=512)\n",
    "    train_ratio = 0.8\n",
    "    batch_size = 512 # Batch over number of games (each game has 59 max moves played)\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    def my_collate_fn(batch):\n",
    "        xstack = []\n",
    "        ystack = []\n",
    "        for batchit in batch:\n",
    "            xstack.append(batchit[0])\n",
    "            ystack.append(batchit[1])\n",
    "        \n",
    "        x =  torch.cat(xstack, dim=0)\n",
    "        y = torch.cat(ystack, dim=0)\n",
    "        return (x,y)\n",
    "\n",
    "    train_size = int(train_ratio * len(training_data))\n",
    "    test_size = len(training_data) - train_size\n",
    "    train_dataset, test_dataset = random_split(training_data, [train_size, test_size])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    test_iter = iter(test_dataloader)\n",
    "    optimizer = optim.Adam(probe.parameters(), lr=learning_rate)\n",
    "    training_loss_history = []\n",
    "    testing_loss_history = []\n",
    "    testing_board_accuracy = []\n",
    "\n",
    "    print(f\"Training Model Name: {model_name}; training/test set size {train_size}/{test_size}\")\n",
    "    if os.path.exists(savepath + model_name + \".ckpt\"):\n",
    "        probe.load_state_dict(torch.load(savepath + model_name + \".ckpt\"))\n",
    "        print(f\"Loaded model checkpopint from {savepath + model_name + '.ckpt'}\")\n",
    "        with open(savepath + model_name + \".pickle\", 'rb') as fhandle:\n",
    "            training_history = pickle.load(fhandle)\n",
    "            training_loss_history = training_history[\"training_loss\"]\n",
    "            testing_loss_history = training_history[\"testing_loss\"]\n",
    "            testing_board_accuracy = training_history[\"testing_board_acc\"]\n",
    "    \n",
    "    i = 0 if len(training_loss_history)==0 else len(training_loss_history)\n",
    "    for epoch in range(num_epochs):\n",
    "        for (x,y) in tqdm(train_dataloader):\n",
    "            if i > train_to: # Resume at an interrupted step\n",
    "                break\n",
    "\n",
    "            ### Train the Probe\n",
    "            probe.train()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits, loss = probe(x, y)\n",
    "            train_loss = loss.item()\n",
    "            training_loss_history.append(train_loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### Compute the error on validation batch\n",
    "            probe.eval()\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    x,y = next(test_iter)\n",
    "                except:\n",
    "                    test_iter = iter(test_dataloader)\n",
    "                    x,y = next(test_iter)\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)        \n",
    "                logits, loss = probe(x,y)\n",
    "                test_loss = loss.item()\n",
    "                testing_loss_history.append(test_loss)\n",
    "\n",
    "                y_hat = torch.argmax(logits, dim=-1, keepdim=False)  # [B, #task]\n",
    "                mean_board_acc = torch.mean(torch.sum(y_hat == y ,-1)/64).item()\n",
    "                testing_board_accuracy.append(mean_board_acc)\n",
    "\n",
    "            ### Print update\n",
    "            i = i + 1\n",
    "            if verbose:\n",
    "                print(f'step [{i+1}/{len(train_dataloader)}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Avg. Board Acc. Test {mean_board_acc:.3f}')\n",
    "\n",
    "            if (i+1) % save_at_steps == 0:\n",
    "                torch.save(probe.state_dict(), savepath + model_name + \".ckpt\")\n",
    "                training_history = {\"training_loss\": training_loss_history, \"testing_loss\": testing_loss_history, \"testing_board_acc\": testing_board_accuracy}\n",
    "                with open(savepath + model_name + \".pickle\", 'wb') as fhandle:\n",
    "                    pickle.dump(training_history, fhandle)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dean_GPTv2_Synthetic_1L1H', 'Dean_GPTv2_Synthetic_1L8H', 'Dean_GPTv2_Synthetic_4L1H', 'Dean_GPTv2_Synthetic_4L8H', 'Dean_GPTv2_Synthetic_8L8H']\n",
      "1 1 Dean_GPTv2_Synthetic_1L1H\n",
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_1L1H/GPT_Synthetic_1Layers_1Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_1L1H_GPT_Layer1; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [31:22<22:30,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8 Dean_GPTv2_Synthetic_1L8H\n",
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_1L8H/GPT_Synthetic_1Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_1L8H_GPT_Layer1; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [29:47<21:22,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1 Dean_GPTv2_Synthetic_4L1H\n",
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_4L1H/GPT_Synthetic_4Layers_1Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_4L1H_GPT_Layer1; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [29:31<21:10,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_4L1H/GPT_Synthetic_4Layers_1Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_4L1H_GPT_Layer2; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [35:41<25:36,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_4L1H/GPT_Synthetic_4Layers_1Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_4L1H_GPT_Layer3; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [42:45<30:39,  2.56s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_4L1H/GPT_Synthetic_4Layers_1Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_4L1H_GPT_Layer4; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [48:35<34:51,  2.91s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8 Dean_GPTv2_Synthetic_4L8H\n",
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_4L8H/GPT_Synthetic_4Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_4L8H_GPT_Layer1; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [29:54<21:27,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_4L8H/GPT_Synthetic_4Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_4L8H_GPT_Layer2; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [36:49<26:25,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_4L8H/GPT_Synthetic_4Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_4L8H_GPT_Layer3; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [43:33<31:14,  2.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_4L8H/GPT_Synthetic_4Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_4L8H_GPT_Layer4; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [50:22<36:07,  3.02s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8 Dean_GPTv2_Synthetic_8L8H\n",
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_8L8H_GPT_Layer1; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [29:47<21:22,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_8L8H_GPT_Layer2; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [36:55<26:29,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_8L8H_GPT_Layer3; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [43:39<31:18,  2.62s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_8L8H_GPT_Layer4; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [50:11<36:00,  3.01s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_8L8H_GPT_Layer5; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [56:41<40:40,  3.40s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_8L8H_GPT_Layer6; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [1:03:10<45:19,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_8L8H_GPT_Layer7; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [1:11:13<51:05,  4.27s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT Model from:  ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training Model Name: linearProbe_Map_New_8L8H_GPT_Layer8; training/test set size 879952/219989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1001/1719 [1:20:20<57:37,  4.82s/it]  \n"
     ]
    }
   ],
   "source": [
    "datapath = \"../EWOthello/ckpts/\"\n",
    "all_files = os.listdir(datapath)\n",
    "GPT_Folders = [name for name in all_files if name.startswith(\"Dean_GPTv2_Synthetic\")]\n",
    "GPT_Folders.sort()\n",
    "print(GPT_Folders)\n",
    "\n",
    "for fold in GPT_Folders:\n",
    "    savepath = datapath + fold + \"/\"\n",
    "    n_layer = int(fold[-4:-3])\n",
    "    n_head = int(fold[-2:-1])\n",
    "    print(n_layer, n_head, fold)\n",
    "    \n",
    "    gpt_ckpt_path = f'../EWOthello/ckpts/Dean_GPTv2_Synthetic_{n_layer}L{n_head}H/GPT_Synthetic_{n_layer}Layers_{n_head}Heads.ckpt'\n",
    "    mconf = GPTConfig(game_dataset.vocab_size, game_dataset.block_size, n_layer=n_layer, n_head=n_head, n_embd=512)\n",
    "\n",
    "    for probe_layer in np.arange(1, n_layer+1, 1):\n",
    "        training_data = probe_dataset(mconf, gpt_ckpt_path, game_dataset, probe_layer)\n",
    "        model_name = f\"linearProbe_Map_New_{n_layer}L{n_head}H_GPT_Layer{probe_layer}\"\n",
    "        train_linear_probe(training_data, model_name, probe_layer, savepath, property_type=\"New\", num_epochs=1, train_to=1000, save_at_steps=100, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14c744919331beb2ed7b4a87042e65cd23febea9f8c117eaa7d3ea2eb03b6521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
