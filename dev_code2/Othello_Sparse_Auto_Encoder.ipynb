{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and specify local GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "import EWOthello.utils.plot_helpers as plt_util\n",
    "from EWOthello.data.othello import *\n",
    "from EWOthello.mingpt.dataset import ProbingDataset, CharDataset # AK's mingpt data child \n",
    "from EWOthello.mingpt.model import GPT, GPTConfig, GPTforProbing, GPTforProbing_v2\n",
    "from EWOthello.mingpt.probe_trainer import Trainer, TrainerConfig\n",
    "from EWOthello.mingpt.utils import set_seed, sample # AKs helpers for sampling predictions\n",
    "from EWOthello.mingpt.probe_model import BatteryProbeClassification\n",
    "set_seed(44)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "# device = \"mps\"\n",
    "device = torch.cuda.current_device()\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generation of Probe Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max num files: 230; Use_num: 1\n",
      "['gen10e5__20220324_155234.pickle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mem Used: 0.2633 GB: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n",
      "Deduplicating finished with 100000 games left\n",
      "Using 20 million for training, 0 for validation\n",
      "Dataset created has 100000 sequences, 61 unique words.\n"
     ]
    }
   ],
   "source": [
    "othello = get(ood_num=-1, data_root=None, num_preload=1) # 11 corresponds to over 1 million games\n",
    "game_dataset = CharDataset(othello) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=0'>1</a>\u001b[0m mconf \u001b[39m=\u001b[39m GPTConfig(game_dataset\u001b[39m.\u001b[39mvocab_size, game_dataset\u001b[39m.\u001b[39mblock_size, n_layer\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, n_head\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, n_embd\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=1'>2</a>\u001b[0m model_probe \u001b[39m=\u001b[39m GPTforProbing(mconf, probe_layer\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=2'>3</a>\u001b[0m model_probe\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m../EWOthello/ckpts/DeanKLi_GPT_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m))   \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=3'>4</a>\u001b[0m model_probe \u001b[39m=\u001b[39m model_probe\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zechenzhang/Documents/Research/Collaborations/Emergent-World-Representations-Othello/dev_code2/train_linear_probes_GPT.ipynb#ch0000004?line=5'>6</a>\u001b[0m properties_modifier_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m59\u001b[39m, \u001b[39m64\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:152\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 152\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    153\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    154\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:136\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    133\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    139\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    140\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(game_dataset.vocab_size, game_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model_probe = GPTforProbing(mconf, probe_layer=6)\n",
    "model_probe.load_state_dict(torch.load(\"../EWOthello/ckpts/DeanKLi_GPT_Synthetic_1L8H/GPT_Synthetic_1Layers_8Heads.ckpt\"))   \n",
    "model_probe = model_probe.to(device)\n",
    "\n",
    "properties_modifier_matrix = np.ones((59, 64))\n",
    "for i in range(59):\n",
    "    if i % 2 == 1:\n",
    "        properties_modifier_matrix[i, :] *= -1.0\n",
    "\n",
    "property_container_v2 = []\n",
    "property_container = []\n",
    "act_container = []\n",
    "for x, _ in tqdm(game_dataset):\n",
    "    # Convert the game index sequence to board number sequence for use with the othello board class\n",
    "    tbf = [game_dataset.itos[_] for _ in x.tolist()]\n",
    "    valid_until = tbf.index(-100) if -100 in tbf else 999\n",
    "\n",
    "    # Get the board state vectors\n",
    "    a = OthelloBoardState()\n",
    "    properties = a.get_gt(tbf[:valid_until], \"get_state\")\n",
    "    property_container.extend(properties)\n",
    "    properties_v2 = (np.array(properties) - 1.0) * properties_modifier_matrix[:valid_until, :] + 1.0\n",
    "    property_container_v2.extend(properties_v2.tolist())\n",
    "\n",
    "    # Get the activation vectors\n",
    "    act = model_probe(x[None, :].to(device))[0].detach().cpu().split(1, dim=0)    \n",
    "    act_container.extend(act)\n",
    "    break\n",
    "\n",
    "### Visualize\n",
    "num_disp = 5\n",
    "fig = plt.figure(figsize=(num_disp*2, 4))\n",
    "ax = plt_util.addAxis(fig, 2, num_disp)\n",
    "for i in range(num_disp):\n",
    "    plt_util.plot_game_discs(np.reshape(property_container[i],[8,8]), ax[i])\n",
    "    plt_util.plot_game_discs(np.reshape(property_container_v2[i], [8,8]), ax[i+num_disp])\n",
    "    ax[i].set_title(f\"Moves Played: {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "ax[0].set_ylabel(\"Board State (Old)\")\n",
    "ax[num_disp].set_ylabel(\"New Representation\")\n",
    "plt_util.format_ax_boardImage(ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14c744919331beb2ed7b4a87042e65cd23febea9f8c117eaa7d3ea2eb03b6521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
